{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanli499/APS360-Project/blob/Lucy_1/VGG_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t6NUyZ_hqq",
        "colab_type": "text"
      },
      "source": [
        "YLL Test Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLljEYqZAWVz",
        "colab_type": "code",
        "outputId": "d0ede1c1-0fb4-4d6d-dff2-5617e97683da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# NEVER RUN THIS AGAIN!!!\n",
        "\n",
        "# logic for sorting thru photos for the ones we want\n",
        "#only run this code once \n",
        "\"\"\"\n",
        "- eg file name: AF01ANFL.JPG\n",
        "- Check:\n",
        "    - length of name = 7, for straight profile only\n",
        "    - str[4:5] = {\"AF\":\"afraid\", \"AN\":\"angry\", \"DI\":\"disgusted\", \"HA\":\"happy\",\n",
        "    \"NE\":\"neutral\", \"SA\":sad\", \"SU\":\"surprised\"}\n",
        "\"\"\"\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# mount our Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# classes are folders in each directory with these names\n",
        "classes = ['afraid','angry','disgusted','happy','neutral','sad','surprised']\n",
        "\n",
        "\n",
        "emotion_code = {\"AF\":\"afraid\", \"AN\":\"angry\", \"DI\":\"disgusted\", \"HA\":\"happy\", \n",
        "                \"NE\":\"neutral\", \"SA\":\"sad\", \"SU\":\"surprised\"}\n",
        "\n",
        "data_dir='/content/drive/My Drive/Colab Notebooks/Faces'\n",
        "\n",
        "# delete existing folder\n",
        "if os.path.exists(data_dir+'/'):\n",
        "    shutil.rmtree(data_dir+'/')\n",
        "\n",
        "try:\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/'+classes[i])\n",
        "\n",
        "    os.mkdir(data_dir+'/train')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/train/'+classes[i])\n",
        "    \n",
        "    os.mkdir(data_dir+'/val')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/val/'+classes[i])\n",
        "\n",
        "    os.mkdir(data_dir+'/test')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/test/'+classes[i])\n",
        "\n",
        "except OSError:\n",
        "    print (\"Creation of the directories failed!\")\n",
        "else:\n",
        "    print (\"Successfully created the directories!\")\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# rootdir = path to KDEF main folder\n",
        "rootdir = '/content/drive/My Drive/Colab Notebooks/PROJECT/KDEF/'\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "    for file in files:\n",
        "        filename = subdir + os.sep + file\n",
        "        if file.endswith(\"S.jpg\") or file.endswith(\"S.JPG\"): \n",
        "            # now crop and modify each image \n",
        "            # get + clean up some data\n",
        "            img = Image.open(filename).convert('L')\n",
        "            new_img = img.resize((224, 224)) # for input into feature layer\n",
        "            if (file[4:6] == \"AF\"): \n",
        "                # Move a file from the directory directory1 to directory2\n",
        "                #shutil.move(directory+filename, data_dir+'/'+emotion_code[\"AF\"]+'/'+filename)\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"AF\"]+'/'+file)\n",
        "            elif (file[4:6] == \"AN\"): \n",
        "                # Move a file from the directory directory1 to directory2\n",
        "                #shutil.move(directory+filename, data_dir+'/'+emotion_code[\"AN\"]+'/'+filename)\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"AN\"]+'/'+file)\n",
        "            elif (file[4:6] == \"DI\"): \n",
        "                # Move a file from the directory directory1 to directory2\n",
        "                #shutil.move(directory+filename, data_dir+'/'+emotion_code[\"DI\"]+'/'+filename)\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"DI\"]+'/'+file)\n",
        "            elif (file[4:6] == \"HA\"): \n",
        "                # Move a file from the directory directory1 to directory2\n",
        "                #shutil.move(directory+filename, data_dir+'/'+emotion_code[\"HA\"]+'/'+filename)\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"HA\"]+'/'+file)\n",
        "            elif (file[4:6] == \"NE\"):\n",
        "                # Move a file from the directory directory1 to directory2\n",
        "                #shutil.move(directory+filename, data_dir+'/'+emotion_code[\"NE\"]+'/'+filename)\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"NE\"]+'/'+file)\n",
        "            elif (file[4:6] == \"SA\"): \n",
        "                # Move a file from the directory directory1 to directory2\n",
        "                #shutil.move(directory+filename, data_dir+'/'+emotion_code[\"SA\"]+'/'+filename)\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"SA\"]+'/'+file)\n",
        "            elif (file[4:6] == \"SU\"): \n",
        "                # Move a file from the directory directory1 to directory2\n",
        "                #shutil.move(directory+filename, data_dir+'/'+emotion_code[\"SU\"]+'/'+filename)\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"SU\"]+'/'+file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# don't know how :(\n",
        "# use shutil.move(data_dir+'/'+emotion_code[\"SU\"]+'/'+filename, \n",
        "#                   data_dir+'/train/'+emotion_code[\"SU\"]+'/'+filename)\n",
        "# use some sort of iterator\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Successfully created the directories!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ONtGEPi4oIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now do train test split (60:20:20)\n",
        "# each class = 140 images --> 84 train, 28 val, 28 test\n",
        "import random\n",
        "\n",
        "# mount our Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# classes are folders in each directory with these names\n",
        "classes = ['afraid','angry','disgusted','happy','neutral','sad','surprised']\n",
        "\n",
        "\n",
        "emotion_code = {\"AF\":\"afraid\", \"AN\":\"angry\", \"DI\":\"disgusted\", \"HA\":\"happy\", \n",
        "                \"NE\":\"neutral\", \"SA\":\"sad\", \"SU\":\"surprised\"}\n",
        "\n",
        "data_dir='/content/drive/My Drive/Colab Notebooks/Faces'\n",
        "\n",
        "# divide data into train, val, + test\n",
        "for c in classes:\n",
        "    filepath = data_dir+'/'+c\n",
        "    i = 0\n",
        "    names = []\n",
        "\n",
        "    for file in os.listdir(filepath):\n",
        "        names.append(file)\n",
        "    random.shuffle(names)\n",
        "\n",
        "    for f in range(84):\n",
        "        shutil.move(filepath+'/'+file, data_dir+'/train/'+c+'/'+names[f])\n",
        "        names.remove(names[f])\n",
        "    for f in range(28):\n",
        "        shutil.move(filepath+'/'+file, data_dir+'/val/'+c+'/'+names[f])\n",
        "        names.remove(names[f])\n",
        "    for f in range(28):\n",
        "        shutil.move(filepath+'/'+file, data_dir+'/test/'+c+'/'+names[f])\n",
        "        names.remove(names[f])\n",
        "\n",
        "\n",
        "\n",
        "# define training and test data directories\n",
        "# train_dir = os.path.join(data_dir, '/train/')\n",
        "# val_dir = os.path.join(data_dir, '/val/')\n",
        "# test_dir = os.path.join(data_dir, '/test/')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dwEF8nz--KU",
        "colab_type": "code",
        "outputId": "ca48310e-d166-4daf-89ed-f41b60b587fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import os\n",
        "# import time\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# import torchvision.transforms as transforms\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "# get vgg16 model\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "vgg16.features\n",
        "vgg16.classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:09<00:00, 60.6MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bCJlYni8kel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The VGG-16 is able to classify 1000 different labels; we just need 4 instead.\n",
        "In order to do that we are going replace the last fully connected layer of the \n",
        "model with a new one with 4 output features instead of 1000.\n",
        "\n",
        "In PyTorch, we can access the VGG-16 classifier with model.classifier, \n",
        "which is an 6-layer array. We will replace the last entry.\n",
        "\"\"\"\n",
        "# from https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
        "class_names = [\"some classes\"]\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
        "features.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\n",
        "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z5xq0l0_467",
        "colab_type": "code",
        "outputId": "02d08422-bd4f-407f-eaf2-423b46414e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "# get + clean up some data\n",
        "from PIL import Image\n",
        "img = Image.open('image.png').convert('LA')\n",
        "# crop\n",
        "new_img = img.resize((224, 224)) # for input into feature layer\n",
        "img.save('greyscale.png')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-59bdabd6d5dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# crop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for input into feature layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'greyscale.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt8z2ig7AFYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feed image to model\n",
        "x = \"some img\"\n",
        "x = x.reshape([1, 3, 350, 210]) # add a dimension for batching\n",
        "print(x.shape)\n",
        "features = vgg16.features(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frRAYSOZGQ9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AlexNet Implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# alexnet\n",
        "import torchvision.models\n",
        "\n",
        "torch.manual_seed(1) # set the random seed\n",
        "\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# confirm output from AlexNet feature extraction\n",
        "alexNet = torchvision.models.alexnet(pretrained=True)\n",
        "features = alexNet.features(images)\n",
        "features.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keCDm37IGGIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount our Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOkjHuzpGCs_",
        "colab_type": "text"
      },
      "source": [
        "YLL: Random code, potentially useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHz0DgU4Czqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From: https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
        "def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    avg_loss_val = 0\n",
        "    avg_acc_val = 0\n",
        "    \n",
        "    train_batches = len(dataloaders[TRAIN])\n",
        "    val_batches = len(dataloaders[VAL])\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        loss_train = 0\n",
        "        loss_val = 0\n",
        "        acc_train = 0\n",
        "        acc_val = 0\n",
        "        \n",
        "        vgg.train(True)\n",
        "        \n",
        "        for i, data in enumerate(dataloaders[TRAIN]):\n",
        "            if i % 100 == 0:\n",
        "                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
        "                \n",
        "            # Use half training dataset\n",
        "            if i >= train_batches / 2:\n",
        "                break\n",
        "                \n",
        "            inputs, labels = data\n",
        "            \n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_train += loss.data[0]\n",
        "            acc_train += torch.sum(preds == labels.data)\n",
        "            \n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        print()\n",
        "        # * 2 as we only used half of the dataset\n",
        "        avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n",
        "        avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n",
        "        \n",
        "        vgg.train(False)\n",
        "        vgg.eval()\n",
        "            \n",
        "        for i, data in enumerate(dataloaders[VAL]):\n",
        "            if i % 100 == 0:\n",
        "                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
        "                \n",
        "            inputs, labels = data\n",
        "            \n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss_val += loss.data[0]\n",
        "            acc_val += torch.sum(preds == labels.data)\n",
        "            \n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        avg_loss_val = loss_val / dataset_sizes[VAL]\n",
        "        avg_acc_val = acc_val / dataset_sizes[VAL]\n",
        "        \n",
        "        print()\n",
        "        print(\"Epoch {} result: \".format(epoch))\n",
        "        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
        "        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
        "        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
        "        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
        "        print('-' * 10)\n",
        "        print()\n",
        "        \n",
        "        if avg_acc_val > best_acc:\n",
        "            best_acc = avg_acc_val\n",
        "            best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "        \n",
        "    elapsed_time = time.time() - since\n",
        "    print()\n",
        "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
        "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
        "    \n",
        "    vgg.load_state_dict(best_model_wts)\n",
        "    return vgg"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}