{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YLL_VGG_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanli499/ChatTime/blob/master/YLL_VGG_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t6NUyZ_hqq",
        "colab_type": "text"
      },
      "source": [
        "YLL Test Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ms--CVQ918x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ALL import statements\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTfCIazf_dAb",
        "colab_type": "code",
        "outputId": "ead3edb9-8b66-4bdf-fa49-5852287549e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount our Google Drive\n",
        "# re-run whenever needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDjCaW97_rWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# re-run when needed\n",
        "# classes are folders in each directory with these names\n",
        "classes = ['afraid','angry','disgusted','happy','neutral','sad','surprised']\n",
        "\n",
        "# emotion label for KDEF photos\n",
        "emotion_code = {\"AF\":\"afraid\", \"AN\":\"angry\", \"DI\":\"disgusted\", \"HA\":\"happy\", \n",
        "                \"NE\":\"neutral\", \"SA\":\"sad\", \"SU\":\"surprised\"}\n",
        "\n",
        "data_dir='/content/drive/My Drive/Colab Notebooks/Faces'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLljEYqZAWVz",
        "colab_type": "code",
        "outputId": "bc09f20c-3314-4347-e137-fefa55d198da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# RUN ONLY ONCE!!!\n",
        "\n",
        "# logic for sorting thru KDEF dataset for the images we want\n",
        "\"\"\"\n",
        "- eg file name: AF01ANFL.JPG\n",
        "- Check:\n",
        "    - length of name = 7, for straight profile only, ends with \"S.jpg\"\n",
        "    - str[4:5] = {\"AF\":\"afraid\", \"AN\":\"angry\", \"DI\":\"disgusted\", \"HA\":\"happy\",\n",
        "    \"NE\":\"neutral\", \"SA\":sad\", \"SU\":\"surprised\"}\n",
        "\"\"\"\n",
        "\n",
        "# delete existing folder\n",
        "if os.path.exists(data_dir+'/'):\n",
        "    shutil.rmtree(data_dir+'/')\n",
        "\n",
        "# make new directories for each emotion class + train, val, test\n",
        "try:\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/'+classes[i])\n",
        "\n",
        "    os.mkdir(data_dir+'/train')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/train/'+classes[i])\n",
        "    \n",
        "    os.mkdir(data_dir+'/val')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/val/'+classes[i])\n",
        "\n",
        "    os.mkdir(data_dir+'/test')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/test/'+classes[i])\n",
        "\n",
        "except OSError:\n",
        "    print (\"Creation of the directories failed!\")\n",
        "else:\n",
        "    print (\"Successfully created the directories!\")\n",
        "\n",
        "# rootdir = path to KDEF main folder\n",
        "rootdir = '/content/drive/My Drive/Colab Notebooks/PROJECT/KDEF/'\n",
        "\n",
        "# go thru KDEF data + sort out desired photos\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "    for file in files:\n",
        "        filename = subdir + os.sep + file\n",
        "        if (file.endswith(\"S.jpg\") or file.endswith(\"S.JPG\")): \n",
        "            # for each straight profile photo:\n",
        "            # convert RGB --> Grayscale\n",
        "            # resize to 256 x 256 pixels, b/c will center crop to 224 x 224 later\n",
        "            # then save in the corresponding emotion class folder\n",
        "            img = Image.open(filename).convert('L')\n",
        "            new_img = img.resize((256, 256))\n",
        "\n",
        "            if (file[4:6] == \"AF\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"AF\"]+'/'+file)\n",
        "            elif (file[4:6] == \"AN\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"AN\"]+'/'+file)\n",
        "            elif (file[4:6] == \"DI\"): \n",
        "                new_img.save(data_dir+'/'+emotion_code[\"DI\"]+'/'+file)\n",
        "            elif (file[4:6] == \"HA\"): \n",
        "                new_img.save(data_dir+'/'+emotion_code[\"HA\"]+'/'+file)\n",
        "            elif (file[4:6] == \"NE\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"NE\"]+'/'+file)\n",
        "            elif (file[4:6] == \"SA\"): \n",
        "                new_img.save(data_dir+'/'+emotion_code[\"SA\"]+'/'+file)\n",
        "            elif (file[4:6] == \"SU\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"SU\"]+'/'+file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the directories!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ONtGEPi4oIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data into train, val, test datasets (60:20:20)\n",
        "# each class = 140 images --> 84 train, 28 val, 28 test\n",
        "\n",
        "# divide data into train, val, + test\n",
        "# for each emotion class, get filenames, shuffle, \n",
        "# divide, move to corresponding folders in train, val, test\n",
        "for c in classes:\n",
        "    filepath = data_dir+'/'+c\n",
        "    names = []\n",
        "\n",
        "    for file in os.listdir(filepath):\n",
        "        names.append(file)\n",
        "    random.shuffle(names)\n",
        "\n",
        "    # TODO: Can probably simplify this\n",
        "    count = 0\n",
        "    for name in names:\n",
        "        if(count == 84):\n",
        "            break\n",
        "        else:\n",
        "            # shutil.move(dir1, dir2) = moves file form directory 1 to directory 2\n",
        "            shutil.move(filepath+'/'+name, data_dir+'/train/'+c+'/'+name)\n",
        "            names.remove(name)\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    for name in names:\n",
        "        if(count == 28):\n",
        "            break\n",
        "        else:\n",
        "            shutil.move(filepath+'/'+name, data_dir+'/val/'+c+'/'+name)\n",
        "            names.remove(name)\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    for name in names:\n",
        "        if(count == 28):\n",
        "            break\n",
        "        else:\n",
        "            shutil.move(filepath+'/'+name, data_dir+'/test/'+c+'/'+name)\n",
        "            names.remove(name)\n",
        "            count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWTrH-BE1Jis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# crop all images to 224 x 224 for all datasets\n",
        "# generate image folders + data loaders for train, val, test\n",
        "def generate_train_val_test_datasets(batch_size=30):\n",
        "    data_transform = transforms.Compose([\n",
        "                                        transforms.CenterCrop(224), \n",
        "                                        transforms.ToTensor()\n",
        "                                        ])\n",
        "\n",
        "    image_datasets = {\n",
        "        'train': datasets.ImageFolder(\n",
        "            os.path.join(data_dir, 'train/'), \n",
        "            transform=data_transform\n",
        "        ),\n",
        "        'val': datasets.ImageFolder(\n",
        "            os.path.join(data_dir, 'val/'), \n",
        "            transform=data_transform\n",
        "        ),\n",
        "        'test': datasets.ImageFolder(\n",
        "            os.path.join(data_dir, 'test/'), \n",
        "            transform=data_transform\n",
        "        )\n",
        "    }\n",
        "\n",
        "    data_loaders = {\n",
        "        'train': torch.utils.data.DataLoader(\n",
        "            image_datasets['train'], batch_size=batch_size\n",
        "        ),\n",
        "        'val': torch.utils.data.DataLoader(\n",
        "            image_datasets['val'], batch_size=batch_size\n",
        "        ),\n",
        "        'test': torch.utils.data.DataLoader(\n",
        "            image_datasets['test'], batch_size=batch_size\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # get size of each dataset\n",
        "    dataset_sizes = {\n",
        "        'train': len(image_datasets['train']),\n",
        "        'val': len(image_datasets['val']),\n",
        "        'test': len(image_datasets['test']) \n",
        "    }\n",
        "\n",
        "    return image_datasets, data_loaders, dataset_sizes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2fXKzw-UaLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in data_loader:\n",
        "        \n",
        "        imgs = vgg16.features(imgs) # take images, overwrite w alexnet features, then use the image in our training\n",
        "        # running feature extraction on each batch in each for loop --> inefficient / slow,\n",
        "        # should run on whole data set 1st, save the extracted features in folder, then load for training\n",
        "        \n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        #############################################\n",
        "\n",
        "        output = model(imgs)\n",
        "        \n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw-Zn2dXi3GM",
        "colab_type": "code",
        "outputId": "73b90c04-200c-4d0e-b40e-3fe5261842eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(1) # set the random seed\n",
        "\n",
        "classes = ['afraid','angry','disgusted','happy','neutral','sad','surprised']\n",
        "\n",
        "data_dir='/content/drive/My Drive/Colab Notebooks/Faces/'\n",
        "\n",
        "# get pretrained vgg16 model\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "# generate training datasets\n",
        "batch_size = 256\n",
        "image_datasets, data_loaders, dataset_sizes = generate_train_val_test_datasets(batch_size)\n",
        "\n",
        "train_loader = data_loaders['train']\n",
        "val_loader = data_loaders['val']\n",
        "test_loader = data_loaders['test']\n",
        "\n",
        "try:\n",
        "    os.mkdir(data_dir+'vgg16')\n",
        "\n",
        "    os.mkdir(data_dir+'vgg16/train')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'vgg16/train/'+classes[i])\n",
        "    \n",
        "    os.mkdir(data_dir+'vgg16/val')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'vgg16/val/'+classes[i])\n",
        "\n",
        "    os.mkdir(data_dir+'vgg16/test')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'vgg16/test/'+classes[i])\n",
        "\n",
        "except OSError:\n",
        "    print (\"Creation of the directories failed!\")\n",
        "else:\n",
        "    print (\"Successfully created the directories!\")\n",
        "\n",
        "def save_tensor(dir_name, img, label, img_num):\n",
        "  features = vgg16.features(img)\n",
        "  path='/content/drive/My Drive/Colab Notebooks/Faces/vgg16/'+dir_name\n",
        "  if (label.item() == 0):\n",
        "    torch.save(features, path + '/afraid/features_' + str(img_num) + '.tensor')\n",
        "  if (label.item() == 1):\n",
        "    torch.save(features, path + '/angry/features_' + str(img_num) + '.tensor')\n",
        "  if (label.item() == 2):\n",
        "    torch.save(features, path + '/disgusted/features_' + str(img_num) + '.tensor')\n",
        "  if (label.item() == 3):\n",
        "    torch.save(features, path + '/happy/features_' + str(img_num) + '.tensor')\n",
        "  if (label.item() == 4):\n",
        "    torch.save(features, path + '/neutral/features_' + str(img_num) + '.tensor')\n",
        "  if (label.item() == 5):\n",
        "    torch.save(features, path + '/sad/features_' + str(img_num) + '.tensor')\n",
        "  if (label.item() == 6):\n",
        "    torch.save(features, path + '/surprised/features_' + str(img_num) + '.tensor')\n",
        "  \n",
        "\n",
        "i = 0\n",
        "for img, label in train_loader:\n",
        "    save_tensor('train', img, label, i)\n",
        "    i+=1\n",
        "\n",
        "i = 0\n",
        "for img, label in val_loader:\n",
        "    save_tensor('val', img, label, i)\n",
        "    i+=1\n",
        "\n",
        "i = 0\n",
        "for img, label in test_loader:\n",
        "    save_tensor('test', img, label, i)\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creation of the directories failed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTN8RrwXVMFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data_loaders, batch_size, num_epochs=1):\n",
        "    train_loader = data_loaders['train']\n",
        "    val_loader = data_loaders['val']\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        for imgs, labels in iter(train_loader):\n",
        "          \n",
        "            imgs = torch.from_numpy(imgs.detach().numpy())\n",
        "            print(n)   \n",
        "\n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "                imgs = imgs.cuda()\n",
        "                labels = labels.cuda()\n",
        "            #############################################         \n",
        "              \n",
        "            out = model(imgs)             # forward pass\n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "            train_acc.append(get_accuracy(model, train_loader)) # compute training accuracy \n",
        "            val_acc.append(get_accuracy(model, val_loader))  # compute validation accuracy\n",
        "            n += 1\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A1S6P0UUV50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Artifical Neural Network Architecture\n",
        "# based on architecture of vgg16.classifier,\n",
        "# with same number of layers + hidden units\n",
        "class ANNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANNClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 32)\n",
        "        self.fc2 = nn.Linear(32, 7) # for our 7 emotion classes  \n",
        "\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(-1, 512 * 7 * 7)\n",
        "        activation1 = F.relu(self.fc1(flattened))\n",
        "        output = self.fc2(activation1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dwEF8nz--KU",
        "colab_type": "code",
        "outputId": "1fc7979e-caf0-4708-e1e9-df1336e70026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "# # generate training datasets\n",
        "# batch_size = 256\n",
        "# image_datasets, data_loaders, dataset_sizes = generate_train_val_test_datasets(batch_size)\n",
        "\n",
        "# # get pretrained vgg16 model\n",
        "# vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "# train model with transfer learning\n",
        "model = ANNClassifier()\n",
        "use_cuda = True\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "start = time.time()\n",
        "train(model, data_loaders, batch_size, num_epochs=2)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Took {} to complete\".format(end - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7536c579e891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train model with transfer learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mANNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
          ]
        }
      ]
    }
  ]
}